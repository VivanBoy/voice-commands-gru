# ğŸ™ï¸ Voice Commands Recognition (Keyword Spotting) â€” MFCC + GRU  
**Cours : Apprentissage profond appliquÃ© avancÃ© â€” Projet final**

Ce projet rÃ©alise un systÃ¨me de **reconnaissance de commandes vocales** (*keyword spotting*) : dÃ©tecter des mots courts (ex. `yes`, `no`, `up`, `down`â€¦) Ã  partir dâ€™un enregistrement audio.  
Pipeline : **Audio (WAV) â†’ Extraction MFCC â†’ ModÃ¨le GRU â†’ PrÃ©diction (Softmax)**, avec des dÃ©monstrations interactives.

---

## âœ… Objectifs du projet
- Construire un pipeline complet de classification audio (prÃ©traitement + entraÃ®nement + Ã©valuation).
- ReconnaÃ®tre des **commandes vocales courtes** (â‰ˆ 1 seconde) parmi des classes ciblÃ©es.
- Fournir des **dÃ©mos** pour montrer lâ€™usage rÃ©el du modÃ¨le :
  - **Gradio** : micro / upload audio â†’ top prÃ©dictions
  - **Turtle** : contrÃ´le dâ€™un curseur/flÃ¨che par la voix

---

## ğŸŒ Importance / Applications rÃ©elles
La reconnaissance de mots-clÃ©s est au cÅ“ur de nombreux usages :
- assistants vocaux (dÃ©clenchement de commandes, â€œwake wordâ€)
- domotique (on/off), IoT, appareils embarquÃ©s
- accessibilitÃ© (contrÃ´le mains libres)
- interfaces vocales offline (faible latence, confidentialitÃ©)

Ce projet correspond Ã  un cas rÃ©el trÃ¨s courant :  
> comprendre une commande simple, vite, sans transcription complÃ¨te de phrases.

---

## ğŸ“¦ DonnÃ©es (Collecte)
### Source : Google Speech Commands Dataset
Dataset public de Google conÃ§u pour le **keyword spotting**, contenant des enregistrements audio de **mots isolÃ©s** prononcÃ©s par de nombreux locuteurs.

**CaractÃ©ristiques gÃ©nÃ©rales :**
- clips courts (â‰ˆ 1 seconde)
- multiples locuteurs / conditions dâ€™enregistrement
- prÃ©sence de bruit de fond (`_background_noise_`) utile pour gÃ©nÃ©rer la classe â€œsilenceâ€ et augmenter la robustesse

### Classes utilisÃ©es dans ce projet
Le projet est construit autour de **12 classes** :
- **10 commandes** : `yes, no, up, down, left, right, on, off, stop, go`
- **unknown** : regroupe tous les autres mots (hors liste)
- **silence** : gÃ©nÃ©rÃ©e Ã  partir de bruit de fond / segments silencieux

---

## ğŸ§ª PrÃ©traitement (Features)
- Resample / standardisation audio (typ. `16 kHz`)
- DurÃ©e fixÃ©e Ã  ~**1 seconde** (padding / trim)
- Extraction **MFCC** :
  - `N_MFCC = 40`
  - sÃ©quence temporelle dâ€™environ `97 frames`
- Normalisation par statistiques du train :
  - `X_norm = (X - mu) / sigma`

> Les scripts de dÃ©mo appliquent le mÃªme prÃ©traitement que lâ€™entraÃ®nement.

---

## ğŸ§  ModÃ¨le (Deep Learning)
Architecture de type :
- **GRU** (rÃ©seau rÃ©current) sur sÃ©quences MFCC
- couches denses + **Softmax** (classification multi-classes)

EntraÃ®nement :
- `Adam` + `SparseCategoricalCrossentropy`
- callbacks de rÃ©gularisation / stabilitÃ© :
  - `EarlyStopping`
  - `ReduceLROnPlateau`
  - `ModelCheckpoint`

ğŸ“Œ RÃ©sultat obtenu (exemple) : **~91% accuracy sur test**.

---

## ğŸ“ Structure du projet
```bash
voice-commands-gru/
â”‚â”€â”€ .gitignore
â”‚â”€â”€ app_gradio.py                  # DÃ©mo web (micro/upload) avec Gradio
â”‚â”€â”€ demo_turtle_voice.py           # DÃ©mo Turtle contrÃ´lÃ©e par la voix
â”‚â”€â”€ reco_vocal_v.ipynb             # Notebook principal (pipeline complet)
â”‚â”€â”€ models/
â”‚   â””â”€â”€ gru_speech_commands.keras  # ModÃ¨le entraÃ®nÃ© (~1MB)
â”‚â”€â”€ data/
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ stats_mfcc40_T97.npz   # (mu, sigma, labels) lÃ©ger, nÃ©cessaire aux dÃ©mos
```

---

## âš™ï¸ Installation (Windows / VS Code)
RecommandÃ© : **Python 3.12** (compatibilitÃ© TensorFlow Windows).

CrÃ©er un environnement virtuel :
```bash
python -m venv .venv
# Windows
.\.venv\Scripts\activate
```

Installer les dÃ©pendances principales :
```bash
pip install --upgrade pip
pip install tensorflow librosa soundfile scikit-learn matplotlib tqdm gradio sounddevice scipy
```

Optionnel (mais pro) :
```bash
pip freeze > requirements.txt
```

---

## â–¶ï¸ Utilisation

### 1) DÃ©mo Gradio (Micro / Upload)
Lancer :
```bash
python app_gradio.py
```

Puis ouvrir lâ€™URL affichÃ©e (ex. `http://127.0.0.1:7860`).

**Notes :**
- la dÃ©mo prend automatiquement la **meilleure seconde** (segment le plus â€œparlÃ©â€) pour Ã©viter le silence au dÃ©but.
- si un mot est hors liste, la prÃ©diction peut tomber sur `unknown`.

---

### 2) DÃ©mo Turtle (contrÃ´le par la voix)
Lancer :
```bash
python demo_turtle_voice.py
```

Commandes (selon la version finale du script) :
- `up/down/left/right` : tourner vers la direction
- `go` : avancer dâ€™un pas
- `on` : commencer Ã  tracer (pen down)
- `off` : arrÃªter de tracer (pen up)
- `yes` : **clear** + recentrer (effacer lâ€™Ã©cran)
- `stop` : quitter

âœ… Exemple dâ€™usage :
1. dire `on` (activer tracÃ©)
2. dire `right` (tourner)
3. dire `go` plusieurs fois (avancer + dessiner)
4. dire `yes` pour effacer

---

## ğŸ§¾ Fichiers volumineux (Important GitHub)
Certaines features peuvent Ãªtre gigantesques si on sauvegarde tout le dataset prÃ©traitÃ©.  
Exemple : `data/processed/speech_mfcc40_T97.npz` peut faire **> 600 MB**.

â¡ï¸ Pour GitHub, on versionne uniquement un fichier lÃ©ger :
- `data/processed/stats_mfcc40_T97.npz` contenant `mu`, `sigma`, `labels`

Le fichier â€œgrosâ€ (si prÃ©sent en local) doit rester ignorÃ© via `.gitignore`.

---

## ğŸš€ AmÃ©liorations possibles
- Data augmentation audio (bruit, time shift, pitch lÃ©ger)
- ModÃ¨le CRNN (CNN + GRU) sur log-mel spectrogrammes
- Export TensorFlow Lite (dÃ©ploiement mobile/edge)
- Vrai temps rÃ©el (fenÃªtre glissante + vote majoritaire)

---

## ğŸ‘¤ Auteur
Projet rÃ©alisÃ© dans le cadre du cours **Apprentissage profond appliquÃ© avancÃ©**  
Reconnaissance de commandes vocales â€” **MFCC + GRU** â€” Speech Commands Dataset
